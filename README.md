# GPT-2-reproduce
Reproducing GPT-2(124M) model by follwing [Andrej Karpathy's lecture](https://www.youtube.com/watch?v=l8pRSuU81PU&t=8096s).
## Basic Explanation
## Papers mentioned in this lecture
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
    - This paper introduces Transformer model.
- [Language Models are Unsupervised Multitask Learners](https://paperswithcode.com/paper/language-models-are-unsupervised-multitask)
    - This is GPT-2 paper.
- [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)
    - This is GPT-3 paper.
- [Gaussian Error Linear Units (GELUs)](https://arxiv.org/abs/1606.08415)
    - This paper introduces GELU function. 
- [FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://arxiv.org/abs/2205.14135)
    - FlashAttention paper introduces a way to calculate attention more efficiently.
- [FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning](https://arxiv.org/abs/2307.08691)
    - Second version of FlashAttention.
- [Online normalizer calculation for softmax](https://arxiv.org/abs/1805.02867)
    - Paper that inspired first FlashAttention paper.