# GPT-2-reproduce
Reproducing GPT-2(124M) model by follwing [Andrej Karpathy's lecture](https://www.youtube.com/watch?v=l8pRSuU81PU&t=8096s).
## Basic Explanation
## Papers mentioned in this lecture
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
- [Language Models are Unsupervised Multitask Learners](https://paperswithcode.com/paper/language-models-are-unsupervised-multitask)
- [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)
- [Gaussian Error Linear Units (GELUs)](https://arxiv.org/abs/1606.08415)
- [FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://arxiv.org/abs/2205.14135)
- [FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning](https://arxiv.org/abs/2307.08691)
- [Online normalizer calculation for softmax](https://arxiv.org/abs/1805.02867)